# agent training parameters
experiment_name: local_actions
env: AbcLocalOperations  # Abc_Env or Mockturtle_Env
algorithm: A2C_local
train_iterations: [40000]
iterations_name: [null]
sgd_minibatch_size: 100
delay_reward_factor: 10
level_reward_factor: 1
normal_reward_factor: 1000
num_sparse_rewards: 99
sparse: True
global_softmax: False

# the circuits for which to run the experiments for
circuits:
  # - adder
  # - bar
  # - log2
  - max
  # - multiplier
  # - sine
  # - sqrt
  # - square

# standard cell library mapping
library_file: libraries/asap7.lib

# available optimizaction actions
optimizations:
  - rewrite
  - rewrite -z
  - refactor
  - refactor -z
  - resub
  - balance
  # - balance -d
  # - desub
  - no_action

# only for Mockturtle relevant
mockturtle:
  graph_type: mig

# delay targets and circuit file locations
target_delays:
  adder: 2000
  bar: 800
  div: 75000
  hype: 1000000
  log2: 7500
  max: 4000
  multiplier: 4000
  sine: 3800
  sqrt: 170000
  square: 2200

circuit_files:
  adder: circuits/adder.v
  bar: circuits/bar.v
  div: circuits/div.v
  hype: circuits/hypo.v
  log2: circuits/log2.v
  max: circuits/max.v
  multiplier: circuits/multiplier.v
  sine: circuits/sine.v
  sqrt: circuits/sqrt.v
  square: circuits/square.v
